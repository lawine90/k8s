import os

os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

import re
import math
import boto3
import torch
import logging
from typing import List, Tuple

from fastapi import FastAPI, Query
from pydantic import BaseModel
from llama_cpp import Llama


# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# --- ì‘ë‹µ ëª¨ë¸ ì •ì˜ ---
class RelkeyResponse(BaseModel):
    q: str
    p: float
    subkeys: List[str]


# --- 1. FastAPI ì•± ì •ì˜ ---
app = FastAPI(title="Qwen Related Query API")

# --- 2. ì „ì—­ ë³€ìˆ˜ ---
model = None
tokenizer = None

# instruction: í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ì§€ì‹œë¬¸ê³¼ ë™ì¼í•œ instruction ì‚¬ìš©
# INSTRUCTION_TEXT = "ë‹¤ìŒ ê²€ìƒ‰ì–´ì™€ ì—°ê´€ëœ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ìƒì„±í•˜ì„¸ìš”."
INSTRUCTION_TEXT = "ë‹¤ìŒ ê²€ìƒ‰ì–´ì™€ ì—°ê´€ëœ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ 'í•œê¸€'ë¡œ ë³€í™˜í•˜ì—¬ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ ìƒì„±í•˜ì„¸ìš”."


# --- 3. API ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ ---
@app.on_event("startup")
def load_model():
    global model, tokenizer

    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
    save_dir = "./model"
    gguf_filename = "qwen-relkey-q4.gguf"

    # MinIO ì‚¬ìš© ì‹œ
    # # K8s ë‚´ë¶€ MinIO ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
    # s3 = boto3.client(
    #     's3',
    #     endpoint_url="http://minio-service.autocomplete.svc.cluster.local:9000",
    #     aws_access_key_id='minioadmin',
    #     aws_secret_access_key='minioadmin',
    #     region_name='us-east-1'
    # )
    #
    # bucket_name = "autocomplete"
    # dir_prefix = "qwen_model/" # MinIO ë‚´ Qwen ëª¨ë¸ í´ë”
    # save_dir = "./downloaded_qwen_model"
    # os.makedirs(save_dir, exist_ok=True)
    #
    # logger.info("--- ğŸ“¥ MinIOì—ì„œ Qwen ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘... ---")
    # try:
    #     response = s3.list_objects_v2(Bucket=bucket_name, Prefix=dir_prefix)
    #     if 'Contents' in response:
    #         for obj in response["Contents"]:
    #             file_key = obj["Key"]
    #             if file_key.endswith('/'): continue
    #
    #             file_name = os.path.basename(file_key)
    #             local_file_path = os.path.join(save_dir, file_name)
    #
    #             # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ (ê°œë°œ ì†ë„ í–¥ìƒ)
    #             if not os.path.exists(local_file_path):
    #                 logger.info(f"Downloading: {file_key}")
    #                 s3.download_file(bucket_name, file_key, local_file_path)
    #     else:
    #         logger.warning(f"âš ï¸ MinIO ë²„í‚·({bucket_name}/{dir_prefix})ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    # except Exception as e:
    #     logger.error(f"âŒ MinIO Error: {e}")
    #     # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì—ëŸ¬ ë¬´ì‹œí•˜ê±°ë‚˜ ê²½ë¡œ ë³€ê²½ í•„ìš”
    #
    # logger.info("--- âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. ëª¨ë¸ ë¡œë”© ì‹œì‘... ---")

    try:
        # # Qwen ëª¨ë¸ ë¡œë“œ
        # tokenizer = AutoTokenizer.from_pretrained(save_dir, trust_remote_code=True)
        # model = AutoModelForCausalLM.from_pretrained(
        #     save_dir,
        #     device_map="auto",      # CPU/GPU ìë™ í• ë‹¹
        #     torch_dtype=torch.float16, # ë©”ëª¨ë¦¬ ìµœì í™”
        #     trust_remote_code=True
        # )
        # gguf model load
        model = Llama(
            model_path=f"{save_dir}/{gguf_filename}",
            n_ctx=256,        # ë¬¸ë§¥ ê¸¸ì´ (ì…ë ¥+ì¶œë ¥)
            n_threads=4,      # CPU ì½”ì–´ ì‚¬ìš© ê°œìˆ˜ (K8s Limitì— ë§ì¶¤)
            n_gpu_layers=0,   # CPU ì „ìš© (GPU ìˆë‹¤ë©´ -1 ë˜ëŠ” ë ˆì´ì–´ ìˆ˜ ì§€ì •)
            verbose=False     # ë¡œê·¸ ë„ê¸° (ì„±ëŠ¥ í–¥ìƒ)
        )
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e

    logger.info(f"--- Qwen ëª¨ë¸ ë¡œë”© ì™„ë£Œ ---")


# --- 4. ì—°ê´€ ê²€ìƒ‰ì–´ ìƒì„± ë¡œì§ ---
# ğŸŒŸ [ì¶”ê°€] ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì •ê·œí™” í•¨ìˆ˜
def normalize_text(text: str) -> str:
    """
    ëŒ€ì†Œë¬¸ì í†µì¼, ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë¹„êµìš©)
    ì˜ˆ: "Pop-Mart" -> "popmart", "POP MART" -> "popmart"
    """
    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()
    # íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì œê±° (ì•ŒíŒŒë²³, í•œê¸€, ìˆ«ìë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^a-z0-9ê°€-í£]', '', text)
    return text


# --- 4. ì—°ê´€ ê²€ìƒ‰ì–´ ìƒì„± ë¡œì§ ---
def generate_keywords(query: str, num_results: int = 10) -> List[str]:
    global model

    prompt = (
        f"### Instruction:\n{INSTRUCTION_TEXT}\n\n"
        f"### Input:\n{query}\n\n"
        f"### Response:\n"
    )

    try:
        # ğŸŒŸ Llama-cpp ì¶”ë¡  ì‹¤í–‰
        output = model(
            prompt,
            max_tokens=64,       # ìƒì„± ê¸¸ì´ ì œí•œ (ì§§ê²Œ)
            stop=["<|endoftext|>", "###", "\n"], # ë©ˆì¶¤ ì¡°ê±´ (í•„ìˆ˜!)
            echo=False,          # í”„ë¡¬í”„íŠ¸ ì œì™¸í•˜ê³  ê²°ê³¼ë§Œ ë°›ìŒ
            temperature=0.1,     # ë‚®ì€ ì˜¨ë„ë¡œ ê³ ì •ëœ ê²°ê³¼ ìœ ë„ (Deterministic)
            top_p=0.9,
            repeat_penalty=1.2   # ë°˜ë³µ ë°©ì§€
        )

        # ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        generated_text = output['choices'][0]['text'].strip()

        raw_keywords = [k.strip() for k in generated_text.split(',') if k.strip()]

        final_keywords = []
        seen = set()
        seen.add(normalize_text(query))  # ìê¸° ìì‹  ì œì™¸

        for k in raw_keywords:
            if len(k) < 2: continue
            norm = normalize_text(k)
            if not norm or norm in seen: continue
            seen.add(norm)
            final_keywords.append(k)

        return final_keywords[:num_results]

    except Exception as e:
        logger.error(f"Inference Error: {e}")
        return []


# --- 5. API ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/api/v1/related/search", response_model=RelkeyResponse)
async def get_related(
        q: str = Query(..., title="Query", min_length=1),
        n: int = Query(5, title="Number of keywords")
):
    """
    Qwen ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—°ê´€ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    keywords = generate_keywords(q, num_results=n)

    return {
        "q": q,
        "p": 0.0,
        "subkeys": keywords
    }


@app.get("/api/v1/related")
def read_root():
    return {"message": "Qwen Related Query API is Ready"}
