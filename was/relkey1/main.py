import re
import os
import math
import boto3
import torch
import logging
from typing import List, Tuple

from fastapi import FastAPI, Query
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# --- ì‘ë‹µ ëª¨ë¸ ì •ì˜ ---
class RelkeyResponse(BaseModel):
    q: str
    p: float
    subkeys: List[str]


# --- 1. FastAPI ì•± ì •ì˜ ---
app = FastAPI(title="Qwen Related Query API")

# --- 2. ì „ì—­ ë³€ìˆ˜ ---
model = None
tokenizer = None

# instruction: í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ì§€ì‹œë¬¸ê³¼ ë™ì¼í•œ instruction ì‚¬ìš©
# INSTRUCTION_TEXT = "ë‹¤ìŒ ê²€ìƒ‰ì–´ì™€ ì—°ê´€ëœ í‚¤ì›Œë“œë¥¼ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ ìƒì„±í•˜ì„¸ìš”."
INSTRUCTION_TEXT = "ë‹¤ìŒ ê²€ìƒ‰ì–´ì™€ ì—°ê´€ëœ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ 'í•œê¸€'ë¡œ ë³€í™˜í•˜ì—¬ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„í•´ ìƒì„±í•˜ì„¸ìš”."


# --- 3. API ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ ---
@app.on_event("startup")
def load_model():
    global model, tokenizer

    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
    save_dir = "./model"

    # MinIO ì‚¬ìš© ì‹œ
    # # K8s ë‚´ë¶€ MinIO ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
    # s3 = boto3.client(
    #     's3',
    #     endpoint_url="http://minio-service.autocomplete.svc.cluster.local:9000",
    #     aws_access_key_id='minioadmin',
    #     aws_secret_access_key='minioadmin',
    #     region_name='us-east-1'
    # )
    #
    # bucket_name = "autocomplete"
    # dir_prefix = "qwen_model/" # MinIO ë‚´ Qwen ëª¨ë¸ í´ë”
    # save_dir = "./downloaded_qwen_model"
    # os.makedirs(save_dir, exist_ok=True)
    #
    # logger.info("--- ğŸ“¥ MinIOì—ì„œ Qwen ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘... ---")
    # try:
    #     response = s3.list_objects_v2(Bucket=bucket_name, Prefix=dir_prefix)
    #     if 'Contents' in response:
    #         for obj in response["Contents"]:
    #             file_key = obj["Key"]
    #             if file_key.endswith('/'): continue
    #
    #             file_name = os.path.basename(file_key)
    #             local_file_path = os.path.join(save_dir, file_name)
    #
    #             # ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ (ê°œë°œ ì†ë„ í–¥ìƒ)
    #             if not os.path.exists(local_file_path):
    #                 logger.info(f"Downloading: {file_key}")
    #                 s3.download_file(bucket_name, file_key, local_file_path)
    #     else:
    #         logger.warning(f"âš ï¸ MinIO ë²„í‚·({bucket_name}/{dir_prefix})ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    # except Exception as e:
    #     logger.error(f"âŒ MinIO Error: {e}")
    #     # ë¡œì»¬ í…ŒìŠ¤íŠ¸ ì‹œì—ëŠ” ì—ëŸ¬ ë¬´ì‹œí•˜ê±°ë‚˜ ê²½ë¡œ ë³€ê²½ í•„ìš”
    #
    # logger.info("--- âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. ëª¨ë¸ ë¡œë”© ì‹œì‘... ---")

    try:
        # Qwen ëª¨ë¸ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(save_dir, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            save_dir,
            device_map="auto",      # CPU/GPU ìë™ í• ë‹¹
            torch_dtype=torch.float16, # ë©”ëª¨ë¦¬ ìµœì í™”
            trust_remote_code=True
        )
    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise e

    logger.info(f"--- Qwen ëª¨ë¸ ë¡œë”© ì™„ë£Œ (Device: {model.device}) ---")


# --- 4. ì—°ê´€ ê²€ìƒ‰ì–´ ìƒì„± ë¡œì§ ---
# ğŸŒŸ [ì¶”ê°€] ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ì •ê·œí™” í•¨ìˆ˜
def normalize_text(text: str) -> str:
    """
    ëŒ€ì†Œë¬¸ì í†µì¼, ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë¹„êµìš©)
    ì˜ˆ: "Pop-Mart" -> "popmart", "POP MART" -> "popmart"
    """
    # ì†Œë¬¸ì ë³€í™˜
    text = text.lower()
    # íŠ¹ìˆ˜ë¬¸ì ë° ê³µë°± ì œê±° (ì•ŒíŒŒë²³, í•œê¸€, ìˆ«ìë§Œ ë‚¨ê¹€)
    text = re.sub(r'[^a-z0-9ê°€-í£]', '', text)
    return text


# --- 4. ì—°ê´€ ê²€ìƒ‰ì–´ ìƒì„± ë¡œì§ ---
def generate_keywords(query: str, num_results: int = 10) -> Tuple[float, List[str]]:
    global model, tokenizer

    prompt = (
        f"### Instruction:\n{INSTRUCTION_TEXT}\n\n"
        f"### Input:\n{query}\n\n"
        f"### Response:\n"
    )

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            do_sample=False,
            num_beams=3,
            early_stopping=True,
            repetition_penalty=1.2,
            eos_token_id=tokenizer.eos_token_id,
            return_dict_in_generate=True,
            output_scores=True
        )

    sequence_prob = 0.0
    if hasattr(outputs, 'sequences_scores'):
        # Log Probabilityì˜ í•©ì´ë¯€ë¡œ expë¥¼ ì·¨í•˜ë©´ í™•ë¥ ì´ ë©ë‹ˆë‹¤.
        # ê°’ì´ ë§¤ìš° ì‘ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìƒí™©ì— ë”°ë¼ ì •ê·œí™”ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        sequence_prob = math.exp(outputs.sequences_scores[0].item())

    output_sequence = outputs.sequences[0]
    full_text = tokenizer.decode(output_sequence, skip_special_tokens=True)

    try:
        if "### Response:\n" in full_text:
            generated_text = full_text.split("### Response:\n")[1].strip()
        else:
            generated_text = full_text

        # 1ì°¨ ë¶„ë¦¬
        raw_keywords = [k.strip() for k in generated_text.split(',') if k.strip()]

        # ğŸŒŸ [ìˆ˜ì •] ì¤‘ë³µ ë° ìœ ì‚¬ ë³€í˜• í•„í„°ë§ ë¡œì§
        final_keywords = []
        seen_normalized = set()

        # ì…ë ¥ ì¿¼ë¦¬ë„ ì •ê·œí™”í•´ì„œ ì œì™¸ ëª©ë¡ì— ì¶”ê°€ (ìê¸° ìì‹  ì¶”ì²œ ë°©ì§€)
        query_normalized = normalize_text(query)
        seen_normalized.add(query_normalized)

        for k in raw_keywords:
            # 1. ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸ (1ê¸€ì)
            if len(k) < 2:
                continue

            # 2. ì •ê·œí™” í›„ ì¤‘ë³µ ê²€ì‚¬
            norm_k = normalize_text(k)

            if not norm_k: # ì •ê·œí™”í–ˆë”ë‹ˆ ë¹ˆ ë¬¸ìì—´ì´ë©´ ì œì™¸
                continue

            if norm_k in seen_normalized:
                continue

            # 3. í¬í•¨ ê´€ê³„ í•„í„°ë§ (ì„ íƒ ì‚¬í•­: "íŒë§ˆíŠ¸"ê°€ ìˆëŠ”ë° "íŒë§ˆíŠ¸ ì½”ë¦¬ì•„"ê°€ ë‚˜ì˜¤ë©´ í—ˆìš©í• ì§€ ë§ì§€)
            # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ 'ì •í™•íˆ ê°™ì€ ë³€í˜•'ë§Œ ë§‰ìŠµë‹ˆë‹¤.

            seen_normalized.add(norm_k)
            final_keywords.append(k)

        return sequence_prob, final_keywords[:num_results]

    except Exception as e:
        logger.warning(f"íŒŒì‹± ì—ëŸ¬: {e}")
        return []


# --- 5. API ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/api/v1/related/search", response_model=RelkeyResponse)
async def get_related(
        q: str = Query(..., title="Query", min_length=1),
        n: int = Query(5, title="Number of keywords")
):
    """
    Qwen ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì—°ê´€ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    prob, keywords = generate_keywords(q, num_results=n)

    return {
        "q": q,
        "p": prob,
        "subkeys": keywords
    }


@app.get("/api/v1/related")
def read_root():
    return {"message": "Qwen Related Query API is Ready"}
