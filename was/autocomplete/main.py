import os
import boto3
import torch
import pytrie
import logging

from enum import Enum
from functools import lru_cache
from fastapi import FastAPI, Query
from pydantic import BaseModel
from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast
from typing import List, Tuple

# --- ë¡œê¹… ì„¤ì • ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# (Pydantic) API ì‘ë‹µ í˜•ì‹ì„ ì •ì˜
class ReturnType(str, Enum):
    FULL = "full"
    token = "token"


class SubkeyResponse(BaseModel):
    subkey: str
    prob: float


class ResultResponse(BaseModel):
    q: str
    subkeys: List[SubkeyResponse]


# --- 1. FastAPI ì•± ë° Pydantic ëª¨ë¸ ì •ì˜ ---
app = FastAPI(title="AI Autocomplete API")

# --- 2. ëª¨ë¸/í† í¬ë‚˜ì´ì € ì „ì—­ ë³€ìˆ˜ ì„ ì–¸ ---
# API ì„œë²„ê°€ ì‹œì‘ë  ë•Œ ë”± í•œ ë²ˆë§Œ ë¡œë“œí•˜ê¸° ìœ„í•œ ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
vocab = {}
choseong_to_ids_map = {}
syllable_trie = None
BPE_SPACE = " "  # Hugging Face í† í¬ë‚˜ì´ì €ì˜ íŠ¹ìˆ˜ ê³µë°± ë¬¸ì (U+2581)

# --- 3. í•œê¸€ ì´ˆì„±(Jamo) ë¶„ë¦¬ í—¬í¼ ---
CHOSEONG_LIST = [
    'ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†',
    'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…'
]
CHOSEONG_SET = set(CHOSEONG_LIST)


def get_choseong(char):
    if 'ê°€' <= char <= 'í£':
        choseong_index = (ord(char) - ord('ê°€')) // (21 * 28)
        return CHOSEONG_LIST[choseong_index]
    elif char in CHOSEONG_SET:
        return char
    else:
        return None


# def quantize_model(model_instance, device):
#     """CPU ì¶”ë¡  ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ëª¨ë¸ì„ 8-bitë¡œ ì–‘ìí™”"""
#     if device == "cpu":
#         try:
#             # torch.quantization.quantize_dynamicì„ ì‚¬ìš©í•˜ì—¬
#             # ëª¨ë¸ì˜ Linear ë ˆì´ì–´ë“¤ì„ int8ë¡œ ë³€í™˜
#             model_quantized = torch.quantization.quantize_dynamic(
#                 model_instance, {torch.nn.Linear}, dtype=torch.qint8
#             )
#             print("--- ğŸ§  (ìµœì í™”) CPU ëª¨ë¸ ë™ì  ì–‘ìí™”(int8) ì ìš© ì™„ë£Œ ---")
#             return model_quantized
#         except Exception as e:
#             print(f"--- âš ï¸ (ê²½ê³ ) ëª¨ë¸ ì–‘ìí™” ì‹¤íŒ¨: {e} ---")
#             return model_instance
#     return model_instance


# --- 4. API ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ ---
@app.on_event("startup")
def load_model_and_vocab():
    """
    FastAPI ì„œë²„ê°€ ì‹œì‘ë  ë•Œ, ëª¨ë¸ê³¼ ì–´íœ˜ì§‘ì„ ì „ì—­ ë³€ìˆ˜(RAM)ì— ë¡œë“œ
    """
    global model, tokenizer, vocab, choseong_to_ids_map, syllable_trie

    # # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
    # save_dir = "./model"
    # print(f"--- AI ëª¨ë¸ ë¡œë”© ì‹œì‘: {save_dir} ---")

    s3 = boto3.client(
        's3',
        endpoint_url="http://minio-service.autocomplete.svc.cluster.local:9000",
        aws_access_key_id='minioadmin',
        aws_secret_access_key='minioadmin',
        region_name='us-east-1'
    )
    bucket_name = "autocomplete"
    dir_prefix = "model/"
    save_dir = "./downloaded_model"
    os.makedirs(save_dir, exist_ok=True)

    print("--- ğŸ“¥ MinIOì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘... ---")
    try:
        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=dir_prefix)
        if 'Contents' in response:
            for obj in response["Contents"]:
                file_key = obj["Key"]
                if file_key.endswith('/'):
                    continue
                local_file_path = os.path.join(save_dir, os.path.basename(file_key))

                logger.info(f"Downloading: {file_key}")
                s3.download_file(bucket_name, file_key, local_file_path)
        else:
            logger.warning("âš ï¸ MinIO ë²„í‚·ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"MinIO Error: {e}")
    print("--- âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. ëª¨ë¸ ë¡œë”© ì‹œì‘... ---")

    tokenizer = PreTrainedTokenizerFast.from_pretrained(save_dir)
    model = AutoModelForCausalLM.from_pretrained(save_dir)

    # GPU or CPU
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # # quantize model
    # model = quantize_model(model_32, device)

    model.to(device)
    model.eval()
    print(f"--- ëª¨ë¸ ë¡œë”© ì™„ë£Œ ({device}) ---")

    # ì–´íœ˜ì§‘ ë° ì´ˆì„± ë§µ êµ¬ì¶•
    vocab = tokenizer.get_vocab()
    print(f"--- ì–´íœ˜ì§‘({len(vocab)}ê°œ) ë¶„ì„ ë° ì´ˆì„± ë§µ êµ¬ì¶• ì¤‘... ---")

    # create vocab with trie
    syllable_trie = pytrie.StringTrie()

    for token_text, token_id in vocab.items():
        if token_text.startswith("##"): continue

        clean_token = token_text[1:] if token_text.startswith(BPE_SPACE) else token_text
        if not clean_token: continue

        first_char = clean_token[0]
        choseong = get_choseong(first_char)

        if choseong:
            if choseong not in choseong_to_ids_map:
                choseong_to_ids_map[choseong] = []
            choseong_to_ids_map[choseong].append(token_id)

        if clean_token in syllable_trie:
            syllable_trie[clean_token].append(token_id)
        else:
            syllable_trie[clean_token] = [token_id]

    print(f"--- ì´ˆì„± ë§µ & Trie êµ¬ì¶• ì™„ë£Œ. API ì„œë²„ ì¤€ë¹„ ì™„ë£Œ ---")


# --- 5. ìë™ì™„ì„± í•µì‹¬ ë¡œì§ ---
@lru_cache(maxsize=2048)
def get_recommendations(
        full_prompt: str,
        num_results: int = 10,
        return_type: str = "full"
) -> List[Tuple[str, float]]:
    """
    input queryë¥¼ ë°›ì•„ì„œ ë‹¤ìŒì— ë‚˜ì˜¬ í† í°ì„ ì œì•ˆ
    """
    global model, tokenizer, vocab, choseong_to_ids_map, syllable_trie

    # (1) Context / Fragment ë¶„ë¦¬
    last_space_index = full_prompt.rfind(" ")
    if last_space_index == -1:
        context = ""
        fragment = full_prompt
    else:
        context = full_prompt[:last_space_index + 1]
        fragment = full_prompt[last_space_index + 1:]

        # (2-1) ëª¨ë¸ ì¶”ë¡ 
    device = model.device # ëª¨ë¸ì´ ë¡œë“œëœ device (cuda or cpu)
    if not context:
        input_ids_tensor = torch.tensor([[tokenizer.bos_token_id]], device=device)
    else:
        inputs = tokenizer(context, return_tensors="pt").to(device)
        input_ids_tensor = inputs.input_ids

    with torch.no_grad():
        outputs = model(input_ids=input_ids_tensor)

    last_token_logits = outputs.logits[0, -1, :]
    all_probabilities = torch.softmax(last_token_logits, dim=-1) # í™•ë¥ ë¡œ ë³€í™˜

    # (2-2) ì»¨í…ìŠ¤íŠ¸ ì¤‘ë³µ í† í° ë¸”ë™ë¦¬ìŠ¤íŠ¸
    blacklist_ids = set(input_ids_tensor[0].tolist())

    # (3) ì–´íœ˜ì§‘(Whitelist) í•„í„°ë§
    whitelist_ids = []
    is_jamo_fragment = (len(fragment) == 1 and fragment in CHOSEONG_SET)

    if is_jamo_fragment:
        # 3.1: ì´ˆì„±(e.g. "ê°•ë‚¨ì—­ ã…")ì¼ ê²½ìš°, ë¯¸ë¦¬ ë§Œë“  'ì´ˆì„± ë§µ' ì‚¬ìš©
        if fragment in choseong_to_ids_map:
            for token_id in choseong_to_ids_map[fragment]:
                if token_id in blacklist_ids: continue
                whitelist_ids.append(token_id)
    else:
        # 3.2: ìŒì ˆ(e.g. 'ê°•ë‚¨ì—­ ë§›')ì¼ ê²½ìš°, 'Trie'ë¡œ prefix ê²€ìƒ‰
        try:
            # "ë§›"ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  í† í° ë¦¬ìŠ¤íŠ¸
            # (ì˜ˆ: [("ë§›ì§‘", [123, 456]), ("ë§›ìˆëŠ”", [789]), ...])
            matches = syllable_trie.items(prefix=fragment)

            for clean_token, token_id_list in matches:
                # 'fragment'ì™€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í† í°ì€ ì œì™¸ (v5 ë¡œì§)
                if clean_token == fragment:
                    continue

                for token_id in token_id_list:
                    if token_id in blacklist_ids: continue
                    whitelist_ids.append(token_id)
        except KeyError:
            pass # Trieì— ì¼ì¹˜í•˜ëŠ” prefixê°€ ì—†ëŠ” ê²½ìš°

    if not whitelist_ids:
        return []

    # (4) í•„í„°ë§
    mask = torch.ones_like(last_token_logits) * -float("Inf")
    mask[whitelist_ids] = 0.0
    filtered_logits = last_token_logits + mask

    # (5) Top-K ì¶”ì¶œ
    top_k_indices = torch.topk(filtered_logits, num_results).indices

    # (6) ê²°ê³¼ ì¡°í•©
    recommendations = []
    context_ids = input_ids_tensor[0]

    for token_id in top_k_indices:
        new_token_id_item = token_id.item()
        if last_token_logits[new_token_id_item] == -float("Inf"):
            continue

        probability = all_probabilities[new_token_id_item].item()

        if return_type == "token":
            # ë‹¨ìˆœíˆ í•´ë‹¹ í† í° ID í•˜ë‚˜ë§Œ ë””ì½”ë”©
            decoded_text = tokenizer.decode([new_token_id_item], skip_special_tokens=True)
            # BPE í† í¬ë‚˜ì´ì €ëŠ” ë‹¨ì–´ ì•ì— ê³µë°±ì„ ë¶™ì´ëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ì œê±°(.strip())
            final_text = decoded_text.strip()
        else:
            new_sequence_ids = torch.cat([context_ids, token_id.unsqueeze(0)], dim=0)
            final_text = tokenizer.decode(new_sequence_ids, skip_special_tokens=True)

        recommendations.append((final_text, probability))

    return recommendations


# --- 6. API ì—”ë“œí¬ì¸íŠ¸ ---
@app.get("/api/v1/search", response_model=ResultResponse)
async def autocomplete(
        q: str = Query(
            ...,
            min_length=1,
            max_length=25,
            title="Search Query",
            description="ìë™ì™„ì„±ì„ ìš”ì²­í•  ê²€ìƒ‰ì–´ ë¬¸ìì—´"
        ),

        n: int = Query(
            default=3,
            title="Number of subkeys",
            description="ë¦¬í„´ë  ë¬¸ìì—´ì˜ ê°œìˆ˜"
        ),

        return_type: ReturnType = Query(
            default=ReturnType.FULL,
            title="Return Type",
            alias="type",
            description="'full'ì´ë©´ ì „ì²´ ë¬¸ì¥, 'token'ì´ë©´ ë§ˆì§€ë§‰ ì œì•ˆë˜ëŠ” ë‹¨ì–´ë§Œ ë°˜í™˜"
        )
):
    """
    GPT-2 ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ ìë™ì™„ì„± ì¶”ì²œ ëª©ë¡ì„ ë°˜í™˜
    """
    # í•µì‹¬ ë¡œì§ í•¨ìˆ˜ í˜¸ì¶œ
    results = get_recommendations(q, num_results=n, return_type=return_type.value)

    # (v7) ê²°ê³¼ë¥¼ API ì‘ë‹µ í˜•ì‹(JSON)ìœ¼ë¡œ ë³€í™˜
    response_data = {
        "q": q,
        "subkeys": [
            SubkeyResponse(subkey=text, prob=prob)
            for text, prob in results
        ]
    }

    return response_data


# --- (ì„ íƒ) ë£¨íŠ¸ ê²½ë¡œ ---
@app.get("/")
def read_root():
    return {"message": "AI Autocomplete API. ' /docs 'ë¡œ ì´ë™í•˜ì—¬ API ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”."}
